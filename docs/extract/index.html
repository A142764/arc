<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-us">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
    <title>Extract - Arc</title>
    <meta name="generator" content="Hugo 0.41" />

    
    <meta name="description" content="Arc is an opinionated framework for defining data pipelines which are predictable, repeatable and manageable.">
    
    <link rel="canonical" href="https://aglenergy.github.io/arc/extract/">
    
    <meta name="author" content="au.com.agl.arc">
    

    <meta property="og:url" content="https://aglenergy.github.io/arc/extract/">
    <meta property="og:title" content="Arc">
    <meta property="og:image" content="https://aglenergy.github.io/arc/images/logo.png">
    <meta name="apple-mobile-web-app-title" content="Arc">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="https://aglenergy.github.io/arc/images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="https://aglenergy.github.io/arc/images/favicon.ico">

    <style>
      @font-face {
        font-family: 'Icon';
        src: url('https://aglenergy.github.io/arc/fonts/icon.eot');
        src: url('https://aglenergy.github.io/arc/fonts/icon.eot')
               format('embedded-opentype'),
             url('https://aglenergy.github.io/arc/fonts/icon.woff')
               format('woff'),
             url('https://aglenergy.github.io/arc/fonts/icon.ttf')
               format('truetype'),
             url('https://aglenergy.github.io/arc/fonts/icon.svg')
               format('svg');
        font-weight: normal;
        font-style: normal;
      }
    </style>

    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/application.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/temporary.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/palettes.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu&#43;Mono">
    <style>
      body, input {
        font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="https://aglenergy.github.io/arc/javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-red palette-accent-teal">



	
	


<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	<nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        Extract
      </div>
    </div>

    

    
    
        
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">
  <a href="https://github.com/aglenergy/arc" class="project">
    <div class="banner">
      
        <div class="logo">
          <img src="https://aglenergy.github.io/arc/images/logo.png">
        </div>
      
      <div class="name">
        <strong>Arc <span class="version">1.0.0</span></strong>
        
          <br>
          aglenergy/arc
        
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      
        <ul class="repo">
          <li class="repo-download">
            <a href="https://github.com/aglenergy/arc/archive/master.zip" target="_blank" title="Download" data-action="download">
              <i class="icon icon-download"></i> Download
            </a>
          </li>
          <li class="repo-stars">
            <a href="https://github.com/aglenergy/arc/stargazers" target="_blank" title="Stargazers" data-action="star">
              <i class="icon icon-star"></i> Stars
              <span class="count">&ndash;</span>
            </a>
          </li>
        </ul>
        <hr>
      

      <div class="toc">
        
        <ul>
          




<li>
  
    



<a  title="Getting started" href="https://aglenergy.github.io/arc/getting-started/">
	
	Getting started
</a>



  
</li>



<li>
  
    



<a  title="Tutorial" href="https://aglenergy.github.io/arc/tutorial/">
	
	Tutorial
</a>



  
</li>



<li>
  
    



<a class="current" title="Extract" href="https://aglenergy.github.io/arc/extract/">
	
	Extract
</a>


<ul id="scrollspy">
</ul>


  
</li>



<li>
  
    



<a  title="Transform" href="https://aglenergy.github.io/arc/transform/">
	
	Transform
</a>



  
</li>



<li>
  
    



<a  title="Load" href="https://aglenergy.github.io/arc/load/">
	
	Load
</a>



  
</li>



<li>
  
    



<a  title="Execute" href="https://aglenergy.github.io/arc/execute/">
	
	Execute
</a>



  
</li>



<li>
  
    



<a  title="Validate" href="https://aglenergy.github.io/arc/validate/">
	
	Validate
</a>



  
</li>



<li>
  
    



<a  title="Metadata" href="https://aglenergy.github.io/arc/metadata/">
	
	Metadata
</a>



  
</li>



<li>
  
    



<a  title="Partials" href="https://aglenergy.github.io/arc/partials/">
	
	Partials
</a>



  
</li>



<li>
  
    



<a  title="Patterns" href="https://aglenergy.github.io/arc/patterns/">
	
	Patterns
</a>



  
</li>



<li>
  
    



<a  title="Contributing" href="https://aglenergy.github.io/arc/contributing/">
	
	Contributing
</a>



  
</li>



<li>
  
    



<a  title="License" href="https://aglenergy.github.io/arc/license/">
	
	License
</a>



  
</li>


        </ul>
        

        
        <hr>
        <span class="section">The author</span>
        
        <ul>
          

          

          
        </ul>
        
      </div>
    </div>
  </div>
</nav>

	</div>

	<article class="article">
		<div class="wrapper">
			<h1>Extract </h1>

			

<p><code>*Extract</code> stages read in data from a database or file system.</p>

<p><code>*Extract</code> stages should meet this criteria:</p>

<ul>
<li>Read data from local or remote filesystems and return a <code>DataFrame</code>.</li>
<li>Do not <a href="../transform">transform/mutate</a> the data.</li>
<li>Allow for <a href="http://www.dbms2.com/2014/07/15/the-point-of-predicate-pushdown/">Predicate Pushdown</a> depending on data source.</li>
</ul>

<h2 id="avroextract">AvroExtract</h2>

<p>The <code>AvroExtract</code> stage reads one or more <a href="https://avro.apache.org/">Apache Avro</a> files and returns a <code>DataFrame</code>.</p>

<h3 id="parameters">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the input Avro files.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td>Can be used to specify a schema in case of no input files. This stage will create an empty <code>dataframe</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>true</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;AvroExtract&quot;,
    &quot;name&quot;: &quot;load customer avro extract&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://input_data/customer/*.avro&quot;,
    &quot;outputView&quot;: &quot;customer&quot;,            
    &quot;persist&quot;: false,
    &quot;authentication&quot;: {
        ...
    },    
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="delimitedextract">DelimitedExtract</h2>

<p>The <code>DelimitedExtract</code> stage reads either one or more delimited text files or an input <code>Dataset[String]</code> and returns a <code>DataFrame</code>. <code>DelimitedExtract</code> will always set the underlying Spark configuration option of <code>inferSchema</code> to <code>false</code> to ensure consistent results.</p>

<h3 id="parameters-1">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>false*</td>
<td>Name of the incoming Spark dataset. If not present <code>inputURI</code> is requred.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>false*</td>
<td>URI of the input delimited text files. If not present <code>inputView</code> is requred.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td>Can be used to specify a schema in case of no input files. This stage will create an empty <code>dataframe</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>delimiter</td>
<td>String</td>
<td>true</td>
<td>The type of delimiter in the file. Supported values: <code>Comma</code>, <code>Pipe</code>, <code>DefaultHive</code>. <code>DefaultHive</code> is  ASCII character 1, the default delimiter for Apache Hive extracts.</td>
</tr>

<tr>
<td>quote</td>
<td>String</td>
<td>true</td>
<td>The type of quoting in the file. Supported values: <code>None</code>, <code>SingleQuote</code>, <code>DoubleQuote</code>.</td>
</tr>

<tr>
<td>header</td>
<td>Boolean</td>
<td>true</td>
<td>Whether or not the dataset contains a header row. If available the output dataset will have named columns otherwise columns will be named <code>_col1</code>, <code>_col2</code> &hellip; <code>_colN</code>.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>true</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-1">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;DelimitedExtract&quot;,
    &quot;name&quot;: &quot;load customer csv extract&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://input_data/customer/*.csv&quot;,
    &quot;outputView&quot;: &quot;customer&quot;,            
    &quot;persist&quot;: false,
    &quot;delimiter&quot;: &quot;Comma&quot;,
    &quot;quote&quot; : &quot;DoubleQuote&quot;,
    &quot;header&quot;: true,
    &quot;authentication&quot;: {
        ...
    },
    &quot;params&quot;: {
    }
}
</code></pre>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;DelimitedExtract&quot;,
    &quot;name&quot;: &quot;split customer record extract&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputView&quot;: &quot;customer_raw&quot;,
    &quot;outputView&quot;: &quot;customer&quot;,            
    &quot;persist&quot;: false,
    &quot;delimiter&quot;: &quot;DefaultHive&quot;,
    &quot;quote&quot; : &quot;SingleQuote&quot;,
    &quot;header&quot;: false,
    &quot;authentication&quot;: {
        ...
    },    
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="httpextract">HTTPExtract</h2>

<p>The <code>HTTPExtract</code> executes a <code>GET</code> request against a remote HTTP service and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-2">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the HTTP server.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>headers</td>
<td>Map[String, String]</td>
<td>false</td>
<td><a href="https://en.wikipedia.org/wiki/List_of_HTTP_header_fields">HTTP Headers</a> to set for the HTTP request. These are not limited to the Internet Engineering Task Force standard headers.</td>
</tr>

<tr>
<td>validStatusCodes</td>
<td>Array[Integer]</td>
<td>false</td>
<td>A list of valid status codes which will result in a successful stage if the list contains the HTTP server response code. If not provided the default values are <code>[200, 201, 202]</code>.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>true</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-2">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;HTTPExtract&quot;,
    &quot;name&quot;: &quot;load customer from customer api&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;http://internalserver/api/customer&quot;,
    &quot;outputView&quot;: &quot;customer&quot;,            
    &quot;persist&quot;: false,
    &quot;headers&quot;: {
        &quot;Authorization&quot;: &quot;Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==&quot;,
        &quot;custom-header&quot;: &quot;payload&quot;,
    },
    &quot;validStatusCodes&quot;: [200],
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="jdbcextract">JDBCExtract</h2>

<p>The <code>JDBCExtract</code> reads directly from a JDBC Database and returns a <code>DataFrame</code>. See <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases">Spark JDBC documentation</a>.</p>

<h3 id="parameters-3">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>jdbcURL</td>
<td>String</td>
<td>true</td>
<td>The JDBC URL to connect to. e.g., <code>jdbc:mysql://localhost:3306</code>. Most common JDBC drivers are in the docker image already.</td>
</tr>

<tr>
<td>tableName</td>
<td>String</td>
<td>true</td>
<td>The JDBC table that should be read. Note that anything that is valid in a <code>FROM</code> clause of a SQL query can be used, e.g. <code>(SELECT * FROM sourcetable WHERE key=value) sourcetable</code> or just <code>sourcetable</code>.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing. This also determines the maximum number of concurrent JDBC connections.</td>
</tr>

<tr>
<td>fetchsize</td>
<td>Integer</td>
<td>false</td>
<td>The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows).</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>true</td>
<td>Map of configuration parameters.. Currently requires <code>user</code> and <code>password</code> to be set here - see example below.</td>
</tr>
</tbody>
</table>

<h3 id="examples-3">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;JDBCExtract&quot;,
    &quot;name&quot;: &quot;extract customer from jdbc&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;outputView&quot;: &quot;ative_customers&quot;,            
    &quot;persist&quot;: false,
    &quot;jdbcURL&quot;: &quot;jdbc:mysql://localhost/mydb&quot;,
    &quot;tableName&quot;: &quot;(SELECT * FROM customer WHERE active=TRUE) customer&quot;,
    &quot;numPartitions&quot;: 10,
    &quot;fetchsize&quot;: 1000,
    &quot;params&quot;: {
        &quot;user&quot;: &quot;mydbuser&quot;,
        &quot;password&quot;: &quot;mydbpassword&quot;,
    }
}
</code></pre>

<h2 id="jsonextract">JSONExtract</h2>

<p>The <code>JSONExtract</code> stage reads either one or more JSON files or an input <code>Dataset[String]</code> and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-4">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>false*</td>
<td>Name of the incoming Spark dataset. If not present <code>inputURI</code> is requred.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>false*</td>
<td>URI of the input delimited text files. If not present <code>inputView</code> is requred.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td>Can be used to specify a schema in case of no input files. This stage will create an empty <code>dataframe</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>multiLine</td>
<td>Boolean</td>
<td>false</td>
<td>Whether the input directory contains a single JSON object per file or multiple JSON records in a single file, one per line (see <a href="http://jsonlines.org/">JSONLines</a>. Default <code>true</code>.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>true</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-4">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;JSONExtract&quot;,
    &quot;name&quot;: &quot;load customer json extract&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://input_data/customer/*.json&quot;,
    &quot;outputView&quot;: &quot;customer&quot;,            
    &quot;persist&quot;: false,
    &quot;multiLine&quot;: true,
    &quot;authentication&quot;: {
        ...
    },
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="orcextract">ORCExtract</h2>

<p>The <code>ORCExtract</code> stage reads one or more <a href="https://orc.apache.org/">Apache ORC</a> files and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-5">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the input ORC files.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td>Can be used to specify a schema in case of no input files. This stage will create an empty <code>dataframe</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>true</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-5">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;ParquetExtract&quot;,
    &quot;name&quot;: &quot;load customer orc extract&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://input_data/customer/*.orc&quot;,
    &quot;outputView&quot;: &quot;customer&quot;,            
    &quot;persist&quot;: false,
    &quot;authentication&quot;: {
        ...
    },    
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="parquetextract">ParquetExtract</h2>

<p>The <code>ParquetExtract</code> stage reads one or more <a href="https://parquet.apache.org/">Apache Parquet</a> files and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-6">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the input Parquet files.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td>Can be used to specify a schema in case of no input files. This stage will create an empty <code>dataframe</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>true</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-6">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;ParquetExtract&quot;,
    &quot;name&quot;: &quot;load customer parquet extract&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://input_data/customer/*.parquet&quot;,
    &quot;outputView&quot;: &quot;customer&quot;,            
    &quot;persist&quot;: false,
    &quot;authentication&quot;: {
        ...
    },    
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="xmlextract">XMLExtract</h2>

<p>The <code>XMLExtract</code> stage reads one or more XML files and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-7">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the input XML files.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td>Can be used to specify a schema in case of no input files. This stage will create an empty <code>dataframe</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>rowTag</td>
<td>String</td>
<td>true</td>
<td>The row tag of your XML files to treat as a row. For example, in this XML: <code>&lt;books&gt;&lt;book&gt;1&lt;/book&gt;&lt;book&gt;2&lt;/book&gt;&lt;/books&gt;</code> the appropriate value would be <code>book</code>.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>true</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-7">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;XMLExtract&quot;,
    &quot;name&quot;: &quot;load customer xml extract&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://input_data/customer/*.xml&quot;,
    &quot;outputView&quot;: &quot;customer&quot;,            
    &quot;persist&quot;: false,
    &quot;rowTag&quot;: &quot;customer&quot;,
    &quot;authentication&quot;: {
        ...
    },    
    &quot;params&quot;: {
    }
}
</code></pre>


			<aside class="copyright" role="note">
				
				&copy; 2018 Released under the MIT license &ndash;
				
				Documentation built with
				<a href="https://www.gohugo.io" target="_blank">Hugo</a>
				using the
				<a href="http://github.com/digitalcraftsman/hugo-material-docs" target="_blank">Material</a> theme.
			</aside>

			<footer class="footer">
				

<nav class="pagination" aria-label="Footer">
  <div class="previous">
  
      <a href="https://aglenergy.github.io/arc/tutorial/" title="Tutorial">
        <span class="direction">
          Previous
        </span>
        <div class="page">
          <div class="button button-previous" role="button" aria-label="Previous">
            <i class="icon icon-back"></i>
          </div>
          <div class="stretch">
            <div class="title">
              Tutorial
            </div>
          </div>
        </div>
      </a>
  
  </div>

  <div class="next">
  
      <a href="https://aglenergy.github.io/arc/transform/" title="Transform">
        <span class="direction">
          Next
        </span>
        <div class="page">
          <div class="stretch">
            <div class="title">
              Transform
            </div>
          </div>
          <div class="button button-next" role="button" aria-label="Next">
            <i class="icon icon-forward"></i>
          </div>
        </div>
      </a>
  
  </div>
</nav>





			</footer>
		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = 'https:\/\/aglenergy.github.io\/arc\/';
      var repo_id  = 'aglenergy\/arc';
    
    </script>

    <script src="https://aglenergy.github.io/arc/javascripts/application.js"></script>
    

    <script>
      /* Add headers to scrollspy */
      var headers   = document.getElementsByTagName("h2");
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            var li = document.createElement("li");
            li.setAttribute("class", "anchor");

            var a  = document.createElement("a");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", headers[i].innerHTML);
            a.innerHTML = headers[i].innerHTML;

            li.appendChild(a)
            scrollspy.appendChild(li);
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }


        /* Add permanent link next to the headers */
        var headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");

        for(var i = 0; i < headers.length; i++) {
            var a = document.createElement("a");
            a.setAttribute("class", "headerlink");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", "Permanent link")
            a.innerHTML = "#";
            headers[i].appendChild(a);
        }
      }
    </script>

    

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

