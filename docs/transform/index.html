<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-us">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
    <title>Transform - Arc</title>
    <meta name="generator" content="Hugo 0.41" />

    
    <meta name="description" content="Arc is an opinionated framework for defining data pipelines which are predictable, repeatable and manageable.">
    
    <link rel="canonical" href="https://aglenergy.github.io/arc/transform/">
    
    <meta name="author" content="au.com.agl.arc">
    

    <meta property="og:url" content="https://aglenergy.github.io/arc/transform/">
    <meta property="og:title" content="Arc">
    <meta property="og:image" content="https://aglenergy.github.io/arc/images/logo.png">
    <meta name="apple-mobile-web-app-title" content="Arc">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="https://aglenergy.github.io/arc/images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="https://aglenergy.github.io/arc/images/favicon.ico">

    <style>
      @font-face {
        font-family: 'Icon';
        src: url('https://aglenergy.github.io/arc/fonts/icon.eot');
        src: url('https://aglenergy.github.io/arc/fonts/icon.eot')
               format('embedded-opentype'),
             url('https://aglenergy.github.io/arc/fonts/icon.woff')
               format('woff'),
             url('https://aglenergy.github.io/arc/fonts/icon.ttf')
               format('truetype'),
             url('https://aglenergy.github.io/arc/fonts/icon.svg')
               format('svg');
        font-weight: normal;
        font-style: normal;
      }
    </style>

    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/application.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/temporary.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/palettes.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu&#43;Mono">
    <style>
      body, input {
        font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="https://aglenergy.github.io/arc/javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-red palette-accent-teal">



	
	


<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	<nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        Transform
      </div>
    </div>

    

    
    <div class="button button-github" role="button" aria-label="GitHub">
      <a href="https://github.com/aglenergy/arc" title="@https://github.com/aglenergy/arc on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
    </div>
    
    
        
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">
  <a href="https://aglenergy.github.io/arc/" class="project">
    <div class="banner">
      
      <div class="logo">
        <img src="https://aglenergy.github.io/arc/images/logo.png">
      </div>
      
      <div class="name">
        <strong>Arc 
          <span class="version">1.0.8</span></strong>
        
        <br> aglenergy/arc 
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      

      <div class="toc">
        
        <ul>
          




<li>
  
    



<a  title="Tutorial" href="https://aglenergy.github.io/arc/tutorial/">
	
	Tutorial
</a>



  
</li>



<li>
  
    



<a  title="Extract" href="https://aglenergy.github.io/arc/extract/">
	
	Extract
</a>



  
</li>



<li>
  
    



<a class="current" title="Transform" href="https://aglenergy.github.io/arc/transform/">
	
	Transform
</a>


<ul id="scrollspy">
</ul>


  
</li>



<li>
  
    



<a  title="Load" href="https://aglenergy.github.io/arc/load/">
	
	Load
</a>



  
</li>



<li>
  
    



<a  title="Execute" href="https://aglenergy.github.io/arc/execute/">
	
	Execute
</a>



  
</li>



<li>
  
    



<a  title="Validate" href="https://aglenergy.github.io/arc/validate/">
	
	Validate
</a>



  
</li>



<li>
  
    



<a  title="Metadata" href="https://aglenergy.github.io/arc/metadata/">
	
	Metadata
</a>



  
</li>



<li>
  
    



<a  title="Partials" href="https://aglenergy.github.io/arc/partials/">
	
	Partials
</a>



  
</li>



<li>
  
    



<a  title="Patterns" href="https://aglenergy.github.io/arc/patterns/">
	
	Patterns
</a>



  
</li>



<li>
  
    



<a  title="Contributing" href="https://aglenergy.github.io/arc/contributing/">
	
	Contributing
</a>



  
</li>



<li>
  
    



<a  title="License" href="https://aglenergy.github.io/arc/license/">
	
	License
</a>



  
</li>


        </ul>
         
        <hr>
        <span class="section">The author</span>

        <ul>
           
          <li>
            <a href="https://github.com/aglenergy" target="_blank" title="@aglenergy on GitHub">
              @aglenergy on GitHub
            </a>
          </li>
           
        </ul>
        
      </div>
    </div>
  </div>
</nav>
	</div>

	<article class="article">
		<div class="wrapper">
			<h1>Transform </h1>

			

<p><code>*Transform</code> stages apply a single transformation to one or more incoming datasets.</p>

<p>Transformers should meet this criteria:</p>

<ul>
<li>Be <a href="https://en.wikipedia.org/wiki/Pure_function">pure</a>.</li>
<li>Perform only a <a href="https://en.wikipedia.org/wiki/Separation_of_concerns">single function</a>.</li>
<li>Utilise Spark <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">internal functionality</a> where possible.</li>
</ul>

<h2 id="difftransform">DiffTransform</h2>

<p>The <code>DiffTransform</code> stage calculates the difference between two input datasets and produces three datasets:</p>

<ul>
<li>A dataset of the <code>intersection</code> of the two datasets - or rows that exist and are the same in both datasets.</li>
<li>A dataset of the <code>left</code> dataset - or rows that only exist in the left input dataset (<code>inputLeftView</code>).</li>
<li>A dataset of the <code>right</code> dataset - or rows that only exist in the right input dataset (<code>inputRightView</code>).</li>
</ul>

<div class="admonition note">
<p class="admonition-title">Persistence</p>
<p>This stage performs this &lsquo;diffing&rsquo; operation in a single pass so if multiple of the output views are going to be used then it is a good idea to set persist = <code>true</code> to reduce the cost of recomputing the difference multiple times.</p>
</div>

<h3 id="parameters">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>inputLeftView</td>
<td>String</td>
<td>true</td>
<td>Name of first incoming Spark dataset.</td>
</tr>

<tr>
<td>inputRightView</td>
<td>String</td>
<td>true</td>
<td>Name of second incoming Spark dataset.</td>
</tr>

<tr>
<td>outputIntersectionView</td>
<td>String</td>
<td>false</td>
<td>Name of output <code>intersection</code> view.</td>
</tr>

<tr>
<td>outputLeftView</td>
<td>String</td>
<td>false</td>
<td>Name of output <code>left</code> view.</td>
</tr>

<tr>
<td>outputRightView</td>
<td>String</td>
<td>false</td>
<td>Name of output <code>right</code> view.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;DiffTransform&quot;,
    &quot;name&quot;: &quot;calculate the difference between the yesterday and today datasets&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputLeftView&quot;: &quot;cutomer_20180501&quot;,            
    &quot;inputRightView&quot;: &quot;cutomer_20180502&quot;,            
    &quot;outputIntersectionView&quot;: &quot;customer_unchanged&quot;,            
    &quot;outputLeftView&quot;: &quot;customer_removed&quot;,            
    &quot;outputRightView&quot;: &quot;customer_added&quot;,            
    &quot;persist&quot;: true,
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="jsontransform">JSONTransform</h2>

<p>The <code>JSONTransform</code> stage transforms the incoming dataset to rows of <code>json</code> strings with the column name <code>value</code>. It is intended to be used before stages like <a href="https://aglenergy.github.io/arc/load/#httpload">HTTPLoad</a> to prepare the data for sending externally.</p>

<h3 id="parameters-1">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-1">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;JSONTransform&quot;,
    &quot;name&quot;: &quot;convert customer data to json&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputView&quot;: &quot;cutomers&quot;,            
    &quot;outputView&quot;: &quot;customersJSON&quot;,            
    &quot;persist&quot;: false,
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="mltransform">MLTransform</h2>

<p>The <code>MLTransform</code> stage transforms the incoming dataset with a pretrained Spark ML (Machine Learning) model. This will append one or more predicted columns to the incoming dataset. The incoming model must be a <code>PipelineModel</code> produced using Spark&rsquo;s Scala, Java, PySpark or SparkR API.</p>

<h3 id="parameters-2">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the input PipelineModel.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count. MLTransform will also log percentiles of prediction probabilities for classification models if this option is enabled.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-2">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;MLTransform&quot;,
    &quot;name&quot;: &quot;apply machine learning model&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://input_data/ml/machineLearningPipelineModel.parquet&quot;,
    &quot;inputView&quot;: &quot;inputDF&quot;,         
    &quot;outputView&quot;: &quot;outputDF&quot;,            
    &quot;persist&quot;: false,
    &quot;authentication&quot;: {
        ...
    },
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="sqltransform">SQLTransform</h2>

<p>The <code>SQLTransform</code> stage transforms the incoming dataset with a <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> statement. This stage relies on previous stages to load and register the dataset views (<code>outputView</code>) and will execute arbitrary SQL statements against those datasets.</p>

<div class="admonition note">
<p class="admonition-title">CAST vs TypingTransform</p>
<p><p>It is strongly recommended to use the <code>TypingTransform</code> for reproducible, repeatable results.</p>

<p>Whilst SQL is capable of converting data types using the <code>CAST</code> function (e.g. <code>CAST(dateColumn AS DATE)</code>) be very careful. ANSI SQL specifies that any failure to convert then an exception condition is raised: <code>data exception-invalid character value for cast</code> whereas Spark SQL will return a null value and suppress any exceptions: <code>try s.toString.toInt catch { case _: NumberFormatException =&gt; null }</code>. If you used a cast in a financial scenario, for example bill aggregation, the silent <code>NULL</code>ing of values could result in errors being suppressed and bills incorrectly calculated.</p>
</p>
</div>

<h3 id="parameters-3">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the input file containing the SQL statement.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>sqlParams</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Parameters to inject into the SQL statement before executing. The parameters use the <code>${}</code> format.<br><br>For example if the sqlParams contains parameter <code>current_timestamp</code> of value <code>2018-11-24 14:48:56</code> then this statement would execute in a deterministic way: <code>SELECT * FROM customer WHERE expiry &gt; FROM_UNIXTIME(UNIX_TIMESTAMP('${current_timestamp}', 'yyyy-MM-dd HH:mm:ss'))</code> (so would be testable).</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-3">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;SQLTransform&quot;,
    &quot;name&quot;: &quot;Join customer and account&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://datalake/sql/0.0.1/customerAccountJoin.sql&quot;,
    &quot;outputView&quot;: &quot;customerAccountDF&quot;,            
    &quot;persist&quot;: false,
    &quot;authentication&quot;: {
        ...
    },    
    &quot;sqlParams&quot;: {
        &quot;current_date&quot;: &quot;2018-11-24&quot;,
        &quot;current_timestamp&quot;: &quot;2018-11-24 14:48:56&quot;
    },    
    &quot;params&quot;: {
    }
}
</code></pre>

<p>The <code>current_date</code> and <code>current_timestamp</code> can easily be passed in as environment variables using <code>$(date &quot;+%Y-%m-%d&quot;)</code> and <code>$(date &quot;+%Y-%m-%d %H:%M:%S&quot;)</code> respectively.</p>

<p>The SQL statement is a plain Spark SQL statement, for example:</p>

<pre><code class="language-sql">SELECT 
    customer.customer_id
    ,customer.first_name
    ,customer.last_name
    ,account.account_id
    ,account.account_name
FROM customer
LEFT JOIN account ON account.customer_id = customer.customer_id
</code></pre>

<h2 id="tensorflowservingtransform">TensorFlowServingTransform</h2>

<div class="admonition note">
<p class="admonition-title">Experimental</p>
<p><p>The <code>TensorFlowServingTransform</code> is currently in experimental state whilst the requirements become clearer.</p>

<p>This means this API is likely to change.</p>
</p>
</div>

<p>The <code>TensorFlowServingTransform</code> stage transforms the incoming dataset by calling a <a href="https://www.tensorflow.org/serving/">TensorFlow Serving</a> service. Because each call is atomic the TensorFlow Serving instances could be behind a load balancer to increase throughput.</p>

<h3 id="parameters-4">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>inputFields</td>
<td>Map[String, String]</td>
<td>true</td>
<td>A map of the fields to be passed to the TensorFlow Serving service. This map describes the required input fields and their type and will be used to extract the correct field from the Spark <code>DataFrame</code> (by name). E.g. a to call a TensorFlow Serving instance which requires an input of <code>customer_usage</code> of type <code>DT_DOUBLE</code> this map will be used to take the field <code>customer_usage</code> from <code>inputView</code> and create the required request of data type <code>DT_DOUBLE</code>. For this stage to work the <code>customer_usage</code> type within Spark must be of <code>DoubleType</code>.</td>
</tr>

<tr>
<td>outputFields</td>
<td>Map[String, String]</td>
<td>true</td>
<td>A map of the fields to be retrieved from the TensorFlow Serving service. This map describes which fields are expected to be returned from the TensorFlow Serving call and will be appended to the <code>outputView</code>.</td>
</tr>

<tr>
<td>hostname</td>
<td>String</td>
<td>true</td>
<td>The hostname of the target service.</td>
</tr>

<tr>
<td>port</td>
<td>Integer</td>
<td>true</td>
<td>The port of the target service.</td>
</tr>

<tr>
<td>modelName</td>
<td>String</td>
<td>true</td>
<td>The name of the TensorFlow Serving model.</td>
</tr>

<tr>
<td>signatureName</td>
<td>String</td>
<td>true</td>
<td>The name of the TensorFlow Serving signature.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-4">Examples</h3>

<pre><code class="language-json">{
    &quot;environments&quot;: [&quot;prd&quot;,&quot;tst&quot;],
    &quot;type&quot;: &quot;TensorFlowServingTransform&quot;,
    &quot;name&quot;: &quot;call the customer segmentation model&quot;,
    &quot;inputView&quot;: &quot;customer&quot;,
    &quot;outputView&quot;: &quot;customer_segmented&quot;,            
    &quot;inputFields&quot;: {
        &quot;customer_usage&quot;: &quot;DT_DOUBLE&quot;
    },
    &quot;outputFields&quot;: {
        &quot;customer_segment&quot;: &quot;DT_INT32&quot;
    },
    &quot;hostname&quot;: &quot;tf&quot;,
    &quot;port&quot;: 9000,
    &quot;modelName&quot;: &quot;simple&quot;, 
    &quot;signatureName&quot;: &quot;serving_default&quot;,
    &quot;persist&quot;: true,
    &quot;params&quot;: {}
}   
</code></pre>

<h2 id="typingtransform">TypingTransform</h2>

<p>The <code>TypingTransform</code> stage transforms the incoming dataset with based on metadata defined in the <a href="../metadata/">metadata</a> format.</p>

<h3 id="parameters-5">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the input file containing the SQL statement.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-5">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;TypingTransform&quot;,
    &quot;name&quot;: &quot;apply data types to customer records&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://datalake/meta/0.0.1/customer_meta.json&quot;,
    &quot;inputView&quot;: &quot;customerUntypedDF&quot;,            
    &quot;outputView&quot;: &quot;customerTypeDF&quot;,            
    &quot;persist&quot;: false,
    &quot;authentication&quot;: {
        ...
    },       
    &quot;params&quot;: {
    }
}
</code></pre>

<h3 id="logical-flow">Logical Flow</h3>

<p>The sequence that these fields are converted from <code>string</code> fields to <code>typed</code> fields is per this flow chart. Each value and its typing metadata is passed into this logical process. For each row the returned <code>values</code> are returned as a table with the returned <code>error</code> values and concatentated into a field called <code>_errors</code>. Patterns for consuming the <code>_errors</code> array is are demonstrated in the <a href="../validate/#sqlvalidate">SQLValidate</a> stage.</p>

<p><img src="https://aglenergy.github.io/arc/img/typing_flow.png" alt="Logical Flow for Data Typing" title="Logical Flow for Data Typing" /></p>


			<aside class="copyright" role="note">
				
				&copy; 2018 Released under the MIT license
				
			</aside>

			<footer class="footer">
				

<nav class="pagination" aria-label="Footer">
  <div class="previous">
  
      <a href="https://aglenergy.github.io/arc/extract/" title="Extract">
        <span class="direction">
          Previous
        </span>
        <div class="page">
          <div class="button button-previous" role="button" aria-label="Previous">
            <i class="icon icon-back"></i>
          </div>
          <div class="stretch">
            <div class="title">
              Extract
            </div>
          </div>
        </div>
      </a>
  
  </div>

  <div class="next">
  
      <a href="https://aglenergy.github.io/arc/load/" title="Load">
        <span class="direction">
          Next
        </span>
        <div class="page">
          <div class="stretch">
            <div class="title">
              Load
            </div>
          </div>
          <div class="button button-next" role="button" aria-label="Next">
            <i class="icon icon-forward"></i>
          </div>
        </div>
      </a>
  
  </div>
</nav>





			</footer>
		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = 'https:\/\/aglenergy.github.io\/arc\/';
      var repo_id  = 'aglenergy\/arc';
    
    </script>

    <script src="https://aglenergy.github.io/arc/javascripts/application.js"></script>
    

    <script>
      /* Add headers to scrollspy */
      var headers   = document.getElementsByTagName("h2");
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            var li = document.createElement("li");
            li.setAttribute("class", "anchor");

            var a  = document.createElement("a");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", headers[i].innerHTML);
            a.innerHTML = headers[i].innerHTML;

            li.appendChild(a)
            scrollspy.appendChild(li);
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }


        /* Add permanent link next to the headers */
        var headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");

        for(var i = 0; i < headers.length; i++) {
            var a = document.createElement("a");
            a.setAttribute("class", "headerlink");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", "Permanent link")
            a.innerHTML = "#";
            headers[i].appendChild(a);
        }
      }
    </script>

    

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

